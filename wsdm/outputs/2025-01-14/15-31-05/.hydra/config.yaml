seed: 6782
debug: false
save_model: true
use_wandb: false
local_rank: 0,1
enable_cuda_optimizations: true
fold: 0
full_fit: true
use_distillation: false
temperature: 1.0
ascend: false
descend: false
dataset:
  input_dataset: data/cot_72b.parquet
model:
  backbone_path: Qwen/Qwen2.5-72B-Instruct
  max_length: 4096
  num_labels: 2
  num_proc: 2
  use_gradient_checkpointing: true
  compile_model: true
  trust_remote_code: false
  attn_implementation: flash_attention_2
  tokenizer:
    truncation_side: left
    use_fast: true
  use_bnb: true
  use_lora: true
  k_shot: 1
  add_fs: true
  lora:
    target_modules:
    - q_proj
    - k_proj
    - v_proj
    - o_proj
    - up_proj
    - down_proj
    - gate_proj
    r: 64
    lora_alpha: 128
    lora_dropout: 0.01
    use_dora: false
    modules_to_save: []
train_params:
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 1
  num_train_epochs: 1
  gradient_accumulation_steps: 2
  warmup_pct: 0.02
  eval_frequency: 400
  patience: 20
optimizer:
  name: AdamW8bit
  lr: 1.0e-06
  lr_lora_a: 1.0e-05
  lr_lora_b: 5.0e-05
  lr_embed_tokens: 1.0e-06
  weight_decay: 0.01
  max_grad_norm: 16.0
  adam_beta_1: 0.9
  adam_beta_2: 0.95
  adam_epsilon: 1.0e-08
outputs:
  model_dir: /disk3/zheng/output/qwen_72b_multi
wandb:
  project: wsdm-dev
  run_name: qwen-listwise-72b
  tags:
  - qwen
